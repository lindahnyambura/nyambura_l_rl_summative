{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0UZIZd9NYMFEFJZoSlCJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lindahnyambura/nyambura_l_rl_summative/blob/main/training/dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wysXlP_H3foF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lindahnyambura/nyambura_l_rl_summative.git\n",
        "%cd nyambura_l_rl_summative\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "GDRIVE = \"/content/gdrive/MyDrive/rl_summative\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSWRWIjri0rX",
        "outputId": "6db85b8f-ef8d-423a-e077-da3af1cb8e03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nyambura_l_rl_summative\n",
        "!git pull origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRFCdFr-A13h",
        "outputId": "90c6a322-edec-4ac9-8afc-0c14e5d61c90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nyambura_l_rl_summative\n",
            "From https://github.com/lindahnyambura/nyambura_l_rl_summative\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Default config**"
      ],
      "metadata": {
        "id": "IrHprkVgsdpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from training.dqn_training import DQNTrainingManager\n",
        "\n",
        "trainer = DQNTrainingManager(\n",
        "    log_dir=f\"{GDRIVE}/logs/dqn\",\n",
        "    model_dir=f\"{GDRIVE}/models/dqn\"\n",
        ")\n",
        "\n",
        "# single config or full search\n",
        "trainer.train_single_config(\n",
        "    \"default\",\n",
        "    trainer.hyperparameter_configs[\"default\"],\n",
        "    total_timesteps=150_000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB8YwYNOjWdP",
        "outputId": "21b52cde-b691-46f4-fbcc-e02c7c408c50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training DQN with configuration: default\n",
            "============================================================\n",
            "Using cuda device\n",
            "Logging to /content/gdrive/MyDrive/rl_summative/logs/dqn/tensorboard/DQN_default_0\n",
            "Eval num_timesteps=7500, episode_reward=584.50 +/- 428.09\n",
            "Episode length: 729.70 +/- 354.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 730      |\n",
            "|    mean_reward      | 585      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.525    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 7500     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00134  |\n",
            "|    n_updates        | 1624     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=15000, episode_reward=973.63 +/- 437.06\n",
            "Episode length: 829.80 +/- 293.76\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 830      |\n",
            "|    mean_reward      | 974      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.0501   |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 15000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000584 |\n",
            "|    n_updates        | 3499     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=22500, episode_reward=854.22 +/- 375.94\n",
            "Episode length: 699.50 +/- 339.10\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 700      |\n",
            "|    mean_reward      | 854      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000581 |\n",
            "|    n_updates        | 5374     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=1053.33 +/- 769.30\n",
            "Episode length: 804.20 +/- 391.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 804      |\n",
            "|    mean_reward      | 1.05e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00493  |\n",
            "|    n_updates        | 7249     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=37500, episode_reward=570.65 +/- 453.08\n",
            "Episode length: 613.30 +/- 332.35\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 613      |\n",
            "|    mean_reward      | 571      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 37500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000687 |\n",
            "|    n_updates        | 9124     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=540.38 +/- 378.78\n",
            "Episode length: 565.20 +/- 345.53\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 565      |\n",
            "|    mean_reward      | 540      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 45000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000579 |\n",
            "|    n_updates        | 10999    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 479      |\n",
            "|    ep_rew_mean      | 475      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 104      |\n",
            "|    time_elapsed     | 459      |\n",
            "|    total_timesteps  | 47946    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000822 |\n",
            "|    n_updates        | 11736    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52500, episode_reward=801.73 +/- 380.04\n",
            "Episode length: 911.10 +/- 193.64\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 911      |\n",
            "|    mean_reward      | 802      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00182  |\n",
            "|    n_updates        | 12874    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=875.11 +/- 569.67\n",
            "Episode length: 745.70 +/- 395.42\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 746      |\n",
            "|    mean_reward      | 875      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000784 |\n",
            "|    n_updates        | 14749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67500, episode_reward=564.14 +/- 633.42\n",
            "Episode length: 573.10 +/- 437.97\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 573      |\n",
            "|    mean_reward      | 564      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 67500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00267  |\n",
            "|    n_updates        | 16624    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=943.62 +/- 490.85\n",
            "Episode length: 912.40 +/- 262.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 912      |\n",
            "|    mean_reward      | 944      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 75000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00296  |\n",
            "|    n_updates        | 18499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82500, episode_reward=802.47 +/- 351.55\n",
            "Episode length: 852.70 +/- 294.61\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 853      |\n",
            "|    mean_reward      | 802      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00197  |\n",
            "|    n_updates        | 20374    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=467.55 +/- 441.33\n",
            "Episode length: 508.10 +/- 429.88\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 508      |\n",
            "|    mean_reward      | 468      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000805 |\n",
            "|    n_updates        | 22249    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 485      |\n",
            "|    ep_rew_mean      | 506      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 103      |\n",
            "|    time_elapsed     | 935      |\n",
            "|    total_timesteps  | 96434    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00247  |\n",
            "|    n_updates        | 23858    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97500, episode_reward=1007.02 +/- 28.18\n",
            "Episode length: 995.90 +/- 12.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 996      |\n",
            "|    mean_reward      | 1.01e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 97500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000667 |\n",
            "|    n_updates        | 24124    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=105000, episode_reward=1085.86 +/- 792.67\n",
            "Episode length: 726.20 +/- 418.42\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 726      |\n",
            "|    mean_reward      | 1.09e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 105000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0183   |\n",
            "|    n_updates        | 25999    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=112500, episode_reward=1933.78 +/- 751.45\n",
            "Episode length: 969.60 +/- 91.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 970      |\n",
            "|    mean_reward      | 1.93e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 112500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000978 |\n",
            "|    n_updates        | 27874    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=120000, episode_reward=585.20 +/- 597.08\n",
            "Episode length: 490.20 +/- 420.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 490      |\n",
            "|    mean_reward      | 585      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 120000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00765  |\n",
            "|    n_updates        | 29749    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=127500, episode_reward=448.41 +/- 445.53\n",
            "Episode length: 447.30 +/- 451.68\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 447      |\n",
            "|    mean_reward      | 448      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 127500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0609   |\n",
            "|    n_updates        | 31624    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=135000, episode_reward=1187.02 +/- 390.40\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | 1.19e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 135000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00242  |\n",
            "|    n_updates        | 33499    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=142500, episode_reward=1028.74 +/- 514.54\n",
            "Episode length: 756.50 +/- 329.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 756      |\n",
            "|    mean_reward      | 1.03e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 142500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000745 |\n",
            "|    n_updates        | 35374    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=1136.90 +/- 451.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | 1.14e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 150000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00133  |\n",
            "|    n_updates        | 37249    |\n",
            "----------------------------------\n",
            "New best model saved! Mean reward: 17.67\n",
            "\n",
            "Training completed for default\n",
            "Training time: 1528.57 seconds\n",
            "Final performance: 17.67 ± 10.20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<stable_baselines3.dqn.dqn.DQN at 0x7de5bdfa9810>,\n",
              " {'config_name': 'default',\n",
              "  'config': {'learning_rate': 0.0001,\n",
              "   'buffer_size': 50000,\n",
              "   'learning_starts': 1000,\n",
              "   'batch_size': 32,\n",
              "   'tau': 1.0,\n",
              "   'gamma': 0.99,\n",
              "   'train_freq': 4,\n",
              "   'gradient_steps': 1,\n",
              "   'target_update_interval': 1000,\n",
              "   'exploration_fraction': 0.1,\n",
              "   'exploration_initial_eps': 1.0,\n",
              "   'exploration_final_eps': 0.05,\n",
              "   'max_grad_norm': 10,\n",
              "   'policy_kwargs': {'net_arch': [256, 256]}},\n",
              "  'training_time': 1528.5667793750763,\n",
              "  'total_timesteps': 150000,\n",
              "  'final_mean_reward': np.float32(17.667057),\n",
              "  'final_std_reward': np.float32(10.195493),\n",
              "  'model_path': '/content/gdrive/MyDrive/rl_summative/models/dqn/dqn_default_20250730_113916_final'})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Fast_Learning Config**"
      ],
      "metadata": {
        "id": "fyy8FScssmz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Fast-learning run with separate logs =====\n",
        "from training.dqn_training import DQNTrainingManager\n",
        "\n",
        "trainer = DQNTrainingManager(\n",
        "    log_dir=f\"{GDRIVE}/logs/dqn_fast\",\n",
        "    model_dir=f\"{GDRIVE}/models/dqn_fast\"\n",
        ")\n",
        "\n",
        "trainer.train_single_config(\n",
        "    \"fast_learning\",\n",
        "    trainer.hyperparameter_configs[\"fast_learning\"],\n",
        "    total_timesteps=150_000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkTSUp2F9tFN",
        "outputId": "a7e7a659-14dd-4858-f72a-0878633ddcb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training DQN with configuration: fast_learning\n",
            "============================================================\n",
            "Using cuda device\n",
            "Logging to /content/gdrive/MyDrive/rl_summative/logs/dqn_fast/tensorboard/DQN_fast_learning_0\n",
            "Eval num_timesteps=7500, episode_reward=807.40 +/- 322.82\n",
            "Episode length: 784.20 +/- 349.93\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 784      |\n",
            "|    mean_reward      | 807      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.677    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 7500     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.000271 |\n",
            "|    n_updates        | 5998     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=15000, episode_reward=593.37 +/- 398.26\n",
            "Episode length: 649.40 +/- 376.67\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 649      |\n",
            "|    mean_reward      | 593      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.353    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 15000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00174  |\n",
            "|    n_updates        | 13498    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22500, episode_reward=833.18 +/- 435.95\n",
            "Episode length: 807.30 +/- 313.29\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 807      |\n",
            "|    mean_reward      | 833      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00147  |\n",
            "|    n_updates        | 20998    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=1531.48 +/- 671.74\n",
            "Episode length: 895.10 +/- 254.34\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 895      |\n",
            "|    mean_reward      | 1.53e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00156  |\n",
            "|    n_updates        | 28498    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=37500, episode_reward=726.77 +/- 367.47\n",
            "Episode length: 810.40 +/- 379.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 810      |\n",
            "|    mean_reward      | 727      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 37500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00382  |\n",
            "|    n_updates        | 35998    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=995.89 +/- 686.05\n",
            "Episode length: 703.90 +/- 355.10\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 704      |\n",
            "|    mean_reward      | 996      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 45000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.000907 |\n",
            "|    n_updates        | 43498    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52500, episode_reward=847.65 +/- 805.47\n",
            "Episode length: 473.40 +/- 306.14\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 473      |\n",
            "|    mean_reward      | 848      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00795  |\n",
            "|    n_updates        | 50998    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=501.92 +/- 363.46\n",
            "Episode length: 563.40 +/- 367.59\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 563      |\n",
            "|    mean_reward      | 502      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00198  |\n",
            "|    n_updates        | 58498    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 601      |\n",
            "|    ep_rew_mean      | 728      |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 746      |\n",
            "|    total_timesteps  | 60095    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00129  |\n",
            "|    n_updates        | 58594    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67500, episode_reward=863.63 +/- 559.44\n",
            "Episode length: 621.20 +/- 359.17\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 621      |\n",
            "|    mean_reward      | 864      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 67500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00142  |\n",
            "|    n_updates        | 65998    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75000, episode_reward=1168.63 +/- 1022.49\n",
            "Episode length: 608.10 +/- 402.04\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 608      |\n",
            "|    mean_reward      | 1.17e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 75000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00182  |\n",
            "|    n_updates        | 73498    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82500, episode_reward=420.28 +/- 536.69\n",
            "Episode length: 424.50 +/- 325.08\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 424      |\n",
            "|    mean_reward      | 420      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.0017   |\n",
            "|    n_updates        | 80998    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=946.37 +/- 322.44\n",
            "Episode length: 912.00 +/- 264.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 912      |\n",
            "|    mean_reward      | 946      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00253  |\n",
            "|    n_updates        | 88498    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97500, episode_reward=944.23 +/- 122.60\n",
            "Episode length: 916.10 +/- 182.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 916      |\n",
            "|    mean_reward      | 944      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 97500    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00381  |\n",
            "|    n_updates        | 95998    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=105000, episode_reward=1233.02 +/- 990.89\n",
            "Episode length: 625.60 +/- 352.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 626      |\n",
            "|    mean_reward      | 1.23e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 105000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00187  |\n",
            "|    n_updates        | 103498   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=112500, episode_reward=1002.21 +/- 787.97\n",
            "Episode length: 586.20 +/- 332.06\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 586      |\n",
            "|    mean_reward      | 1e+03    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 112500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00136  |\n",
            "|    n_updates        | 110998   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=822.61 +/- 743.46\n",
            "Episode length: 737.10 +/- 330.93\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 737      |\n",
            "|    mean_reward      | 823      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 120000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00333  |\n",
            "|    n_updates        | 118498   |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 650      |\n",
            "|    ep_rew_mean      | 1.01e+03 |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 81       |\n",
            "|    time_elapsed     | 1536     |\n",
            "|    total_timesteps  | 125134   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00185  |\n",
            "|    n_updates        | 123632   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=127500, episode_reward=585.81 +/- 561.23\n",
            "Episode length: 471.50 +/- 381.62\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 472      |\n",
            "|    mean_reward      | 586      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 127500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00216  |\n",
            "|    n_updates        | 125998   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=135000, episode_reward=642.22 +/- 204.17\n",
            "Episode length: 381.80 +/- 149.76\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 382      |\n",
            "|    mean_reward      | 642      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 135000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.00345  |\n",
            "|    n_updates        | 133498   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=142500, episode_reward=1273.87 +/- 769.88\n",
            "Episode length: 721.50 +/- 353.14\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 722      |\n",
            "|    mean_reward      | 1.27e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 142500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.0174   |\n",
            "|    n_updates        | 140998   |\n",
            "----------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=1167.64 +/- 713.41\n",
            "Episode length: 743.70 +/- 336.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 744      |\n",
            "|    mean_reward      | 1.17e+03 |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.03     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 150000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0003   |\n",
            "|    loss             | 0.0142   |\n",
            "|    n_updates        | 148498   |\n",
            "----------------------------------\n",
            "New best model saved! Mean reward: 8.67\n",
            "\n",
            "Training completed for fast_learning\n",
            "Training time: 1846.91 seconds\n",
            "Final performance: 8.67 ± 7.51\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<stable_baselines3.dqn.dqn.DQN at 0x7de5b84f2150>,\n",
              " {'config_name': 'fast_learning',\n",
              "  'config': {'learning_rate': 0.0003,\n",
              "   'buffer_size': 75000,\n",
              "   'learning_starts': 1500,\n",
              "   'batch_size': 128,\n",
              "   'tau': 1.0,\n",
              "   'gamma': 0.99,\n",
              "   'train_freq': 2,\n",
              "   'gradient_steps': 2,\n",
              "   'target_update_interval': 750,\n",
              "   'exploration_fraction': 0.15,\n",
              "   'exploration_initial_eps': 1.0,\n",
              "   'exploration_final_eps': 0.03,\n",
              "   'max_grad_norm': 15,\n",
              "   'policy_kwargs': {'net_arch': [384, 384, 192]}},\n",
              "  'training_time': 1846.9134232997894,\n",
              "  'total_timesteps': 150000,\n",
              "  'final_mean_reward': np.float32(8.673488),\n",
              "  'final_std_reward': np.float32(7.511156),\n",
              "  'model_path': '/content/gdrive/MyDrive/rl_summative/models/dqn_fast/dqn_fast_learning_20250730_121839_final'})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation**"
      ],
      "metadata": {
        "id": "FVz8QCfNs0kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "GDRIVE = \"/content/gdrive/MyDrive/rl_summative\"\n",
        "\n",
        "# ----- helper to read SB3 evaluation CSV -----\n",
        "def read_eval(run_dir, label):\n",
        "    csv = Path(run_dir) / \"evaluations.npz\"\n",
        "    data = np.load(csv)\n",
        "    # SB3 stores: results['results'] shape (n_evals, n_episodes)\n",
        "    raw_rewards = data['results'].flatten()\n",
        "    return {\n",
        "        \"label\": label,\n",
        "        \"best_raw\": float(raw_rewards.max()),\n",
        "        \"final_raw\": float(raw_rewards[-20:].mean()),  # last 20 ep\n",
        "        \"std_raw\": float(raw_rewards[-20:].std()),\n",
        "    }\n",
        "\n",
        "# ----- collect -----\n",
        "runs = {\n",
        "    \"Default\": f\"{GDRIVE}/logs/dqn\",\n",
        "    \"Fast-learning\": f\"{GDRIVE}/logs/dqn_fast\",\n",
        "}\n",
        "results = [read_eval(d, l) for l, d in runs.items()]\n",
        "\n",
        "# ----- plot -----\n",
        "labels = [r[\"label\"] for r in results]\n",
        "best   = [r[\"best_raw\"] for r in results]\n",
        "final  = [r[\"final_raw\"] for r in results]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.bar(x - width/2, best, width, label='Best raw')\n",
        "ax.bar(x + width/2, final, width, label='Final raw (last 20)')\n",
        "ax.set_ylabel('Episode reward (raw)')\n",
        "ax.set_title('DQN Hyper-parameter comparison (150 k steps)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "save_path = Path(GDRIVE) / \"plots\" / \"dqn_comparison.png\"\n",
        "save_path.parent.mkdir(exist_ok=True, parents=True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_path, dpi=300)\n",
        "plt.show()\n",
        "print(\"Saved →\", save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "pA7Leog4cEKp",
        "outputId": "8a084b14-0680-4251-dc6f-e2182a743000"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved → /content/gdrive/MyDrive/rl_summative/plots/dqn_comparison.png\n"
          ]
        }
      ]
    }
  ]
}